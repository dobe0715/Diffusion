{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPV+b2ZTuNS4aFqlRhahg2Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Classifier-free Diffusion Guidance(2022)"],"metadata":{"id":"CtNvITu9Ya3F"}},{"cell_type":"markdown","source":["(참고링크)\n","+ youtube : https://www.youtube.com/watch?v=Q_o0SpXv9kU\n","+ blog : https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/cfdg/\n","+ paper : https://arxiv.org/abs/2207.12598"],"metadata":{"id":"0wCvY5fcYa1T"}},{"cell_type":"markdown","source":["## Contributions\n","1. unconditional model을 학습하면서 동시에 conditional model을 학습시킬 수 있다.\n","2. 기존에 비해 학습 파이프라인을 간단화 하였다,"],"metadata":{"id":"_X5NPamAYazO"}},{"cell_type":"markdown","source":["## BackGround"],"metadata":{"id":"LKQcruqnYaxT"}},{"cell_type":"markdown","source":["### Diffusion Models Beat GANs on Image Synthesis(Classifier Guidance 제안)"],"metadata":{"id":"lMomR1KVYavN"}},{"cell_type":"markdown","source":["핵심은, conditional 모델의 likelihood 식을 잘 전개하면, unconditional 모델과 classifier 모델로 나눠 학습시킬 수 있다는 것이다."],"metadata":{"id":"7WCOgRXsYatQ"}},{"cell_type":"markdown","source":["$p_{\\theta, \\phi}(x_t|x_{t+1}, y) \\simeq Zp_{\\theta}(x_t|x_{t+1})p_{\\phi}(y|x_t) \\simeq \\log{p_\\theta(z)} + C_4, z \\sim N(\\mu_\\phi + \\Sigma_\\phi g, \\Sigma_\\phi)$"],"metadata":{"id":"J1qVPfctYarK"}},{"cell_type":"markdown","source":["이를 노이즈 예측 모델로 대응시키면 다음과 같다.  \n","$\\tilde{\\epsilon}(z_\\lambda, c) = \\epsilon_\\theta(z_\\lambda, c) - w \\sigma_\\lambda \\nabla_{x_t}\\log{p_{\\phi}}(c|z_\\lambda)$  \n","</br>\n","$c$ : condition  \n","$z_\\lambda$ : noised 이미지    \n","$w$ : classifier guidance weight    "],"metadata":{"id":"IEJpu5LPib_o"}},{"cell_type":"markdown","source":["w값을 키우면, 모델의 다양성은 줄어들지만, inception score는 증가한다."],"metadata":{"id":"gxs-s9M6paC5"}},{"cell_type":"markdown","source":["__문제점__\n","1. 두개의 모델을 적절하게 학습시켜야 한다는 번거로움\n","2. diffusion sampling을 하는동안에 adversarial attack을 준다고 이해할 수 있다.(온전한 sampling을 못하게 됨.)\n","    + GAN에 대응 시켜볼 수 있다.\n","    + generator -> diffusion model\n","    + discriminator -> classifier\n","\n","이럴꺼면 GAN을 잘 발전시키지, 굳이 diffusion을 사용할 이유가 없어진다."],"metadata":{"id":"SA-vnbizYapC"}},{"cell_type":"markdown","source":["## Classifier-Free Guidance"],"metadata":{"id":"4zOqkHkrYZB_"}},{"cell_type":"markdown","source":["위에서 유도한 식으로부터,논문의 notation으로 바꿔서 써보면   \n","$\\tilde{p_\\theta(z_\\lambda | c)} \\propto p_\\theta(z_\\lambda|c)p_\\theta(c|z_\\lambda)^w$  \n","이고,"],"metadata":{"id":"3oztf759YZAC"}},{"cell_type":"markdown","source":["여기서, 비례관계이기 때문에 w값을 w+1로 바꿔줄 수 있다. 그러면, 이렇게 바꾼 값을 score에서 살펴보면"],"metadata":{"id":"HZExuHHOYY94"}},{"cell_type":"markdown","source":["$\\epsilon_\\theta(z_\\lambda) - (w+1)\\sigma_\\lambda\\nabla_{z_\\lambda}\\log{p_{\\phi}}(c|z_\\lambda)$  \n","$\\simeq -\\sigma_\\lambda \\nabla_{z_\\lambda}[\\log{p(z_\\lambda)} + (w + 1)\\log{p_\\theta (c|z_\\lambda)}]$  \n","$= -\\sigma_\\lambda \\nabla_{z_\\lambda}[\\log{p(z_\\lambda | c)} + w \\log{p_\\theta (c|z_\\lambda)}]$"],"metadata":{"id":"l9u1vQhSYY7u"}},{"cell_type":"markdown","source":["와 같이 최종적으로 정리할 수 있다."],"metadata":{"id":"3DY7mVh4YY5_"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1n-wKgMDpK_VhxknTxUPVgZHAMpSAzSFW\" height=200>"],"metadata":{"id":"EbccXd2UvJpI"}},{"cell_type":"markdown","source":["귀찮았던 classifier의 gradient가 없어졌다..!!  \n","(또한, w=-1이면 ddpm, w=0이면 conditional ddpm)"],"metadata":{"id":"Tjie0reOvJla"}},{"cell_type":"markdown","source":["__결론__  \n","+ training : condition줬을 때의 노이즈와 uncondition일 때의 노이즈를 모두 학습시킨다.   \n","</br>\n","+ sampling : w 가중치를 통해 interpolation한 값을 사용한다."],"metadata":{"id":"tw9RS2FevJiK"}},{"cell_type":"markdown","source":["__Q.그냥 condition 주고 u-net에 학습하면 안되나요? 뭐하러 두가지 나누고 interpolation하죠__"],"metadata":{"id":"rpnzlqsyvJfo"}},{"cell_type":"markdown","source":["__A.기존에 그렇게 conditional ddpm을 해봤는데 학습이 잘 안됐다. 그래서 앞에서처럼 불편하게 classifier를 추가해준 것이다.__"],"metadata":{"id":"aPQNHgW0vJdt"}},{"cell_type":"markdown","source":["최종적인 학습 형태이다.  \n","<img src=\"https://drive.google.com/uc?id=1YJ_vcQDRE4xue1VN6b1WJe_IxwspGzhI\" height=230>"],"metadata":{"id":"YONV-scHvJcC"}},{"cell_type":"markdown","source":["p라는 확률 값을 통해, condition을 랜덤하게 주거나 안주면서 학습시키고, (사실상 원래의 conditional ddpm 코드에 $p$값 추가해주는 한줄 추가해주면 된다.)"],"metadata":{"id":"7LDSKEZzvJaB"}},{"cell_type":"markdown","source":["샘플링 알고리즘이다.  \n","<img src=\"https://drive.google.com/uc?id=13W6-zosvA-uomQa0XvHHHvcy1ynAayBI\" height=280>"],"metadata":{"id":"6pZZnhjVvJX0"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1lKoQ-Aevs2EWox29nqOzggNCPel0b8Sz\" width=700>"],"metadata":{"id":"2Wi_QTPHvJUd"}},{"cell_type":"markdown","source":["즉, conditional 모델에 unconditional모델을 이용해 $w$만큼 guidance를 준다고 볼 수 있다."],"metadata":{"id":"4n_i2IVUvJSH"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1uptoXpcfqdk5vEMvgO4-qNiqjFVlO1sf\" width=600>  \n","w 값을 올릴수록, FID 증가, IS 증가하는 모습을 볼 수 있다.(Trade off)"],"metadata":{"id":"dz7kTRRwYY3p"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1ETzX7C5vm8Naa31pIHTgB-OC5mbIfVl_\" height=300>"],"metadata":{"id":"C2CSKG7XYY1p"}}]}