{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOAJSAsredKkevlQsFZSzjr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Denoising Diffusion Probabilistic Models\n","참고 링크\n","+ Diffusion Model 수학이 포함된 tutorial : https://www.youtube.com/watch?v=uFoGaIVHfoE\n","+ learn open cv에서 올린 포스트 : https://learnopencv.com/denoising-diffusion-probabilistic-models/\n","\n","Paper\n","+ https://arxiv.org/pdf/2006.11239.pdf"],"metadata":{"id":"ELJj6xnLCIQV"}},{"cell_type":"markdown","source":["#### 논문에서의 아이디어\n","+ 데이터에 매우 작은 노이즈를 추가하는 process를 중첩해서 해주면 normal distribution 까지 보낼 수 있다.\n","+ 그리고 이것을 마찬가지로 순차적으로 denoising 해줘 이미지를 복원할 수 있다."],"metadata":{"id":"j4JQB2kADyR7"}},{"cell_type":"markdown","source":["#### 해야할 것\n","1. forward : noise의 중첩을 어떻게 표현할 것인지?\n","2. backward : noising과정을 어떻게 파라미터화 해서 loss를 구하고 backpropa할 것인가?"],"metadata":{"id":"poNmvPoJExgu"}},{"cell_type":"markdown","source":["#### 어떻게?\n","1. markov chain을 가정하여 noise의 중첩을 설명\n","\n","2. variational inference를 이용해 noising을 설명하고, reparameter화 하여서 loss를 구성하였다\n","    + VI란, 구하고자하는 posterior를 계산할 수 없어 간접적으로 알고있는 분포를 통해 lower bound를 최대화 하는 방법이다.\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1DzU3eReh-eNBS8GsO7tVU5hfAuhdqhnj\" width=300>  \n","\n","이제, 그 어떻게에 대해 좀더 자세하게 알아보자"],"metadata":{"id":"UuWtNelXF0b5"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=19lhFdBzXRkc7Z9_0wOffcPXxnfJk0ZL8\">  \n","\n","\n"],"metadata":{"id":"OOW2wW_aCIOS"}},{"cell_type":"markdown","source":["#### 수식 정리\n","+ $x_0$ : 실제 이미지\n","+ $x_t$ : t번의 노이즈가 중첩 된 이미지($x_T$는 normal distribution이 된다고 가정)\n","+ $q(x_t|x_{t-1})$ : forward process, 스케쥴링된 분산값인 $\\beta_t$들에 의해 변화됨.\n","+ $q(x_{t-1}|x_t)$ : q로부터 유도된 denoising함수\n","+ $p_\\theta(x_{t-1}|x_t)$ : 파라미터 $\\theta$를 통해 근사된 denoising함수"],"metadata":{"id":"tdJ7RfXnCIL6"}},{"cell_type":"markdown","source":["## 1. Forward process\n","(목표 : noise 과정을 잘 설명하기)\n"],"metadata":{"id":"YAr5QFqyCIJt"}},{"cell_type":"markdown","source":["+ $q(x_t|x_{t-1}) = N(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t 𝐈)$"],"metadata":{"id":"Qy2nWPAvCIHU"}},{"cell_type":"markdown","source":["+ 이와같이 구성하게 되면 forward process를 효과적으로 표현할 수 있고, 나아가서 reparameterization trick을 통해 backpropagation에도 사용할 수 있다.\n","+ 이것을 중첩하게 되면 어떻게 되는가?"],"metadata":{"id":"EDFtl5dOCIFP"}},{"cell_type":"markdown","source":["$\\alpha_t=1-\\beta_t$  \n","$\\bar{\\alpha_t}=\\alpha_t*\\alpha_{t-1}*\\cdot\\cdot\\cdot*\\alpha_1$  \n","라고 했을 때, 다음이 성립한다.\n","+ $q(x_t|x_0) = N(x_t;\\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha_t})I)$\n"],"metadata":{"id":"q5mF_j6PCIC_"}},{"cell_type":"markdown","source":["(수식증명)  \n","<img src=\"https://drive.google.com/uc?id=18eBqVlXZZAVebxmDT4a0ijMdGRrx9hWg\" width=500>"],"metadata":{"id":"pMQIbT8OCIAu"}},{"cell_type":"markdown","source":["## 2. Backward process with Loss\n","\n","우선, 전통적인 diffusion에 대하여 variational inference를 통한 loss 텀이 다음과 같이 있다.\n","\n","+ $\\mathbb{E}_q\\bigg[D_{KL}(q(x_T|x_0) || p(x_T)) + \\sum_{t>1}{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t)) - \\log(p_\\theta(x_0|x_1))}\\bigg]$"],"metadata":{"id":"llcB36p9CH-e"}},{"cell_type":"markdown","source":["왼쪽부터, $L_T$, $L_{t-1}$, $L_0$라고 이야기한다.\n","+ $L_T$ : 마지막 결과에 대한 regulerization term이라고 볼 수 있다. 하지만, 여기서는 $\\beta_t$들을 통해 스캐쥴링되어있어 결과적으로 나오는 값과 실제 가우스분포와의 차이를 계산하면 항상 동일하게 나와서 상수로 일정해진다. 따라서, 생략이 가능하다.\n","+ $L_0$ : reverse 마지막 하고나서 실제이미지와의 reconstruction term이다. 논문에서는 해당 부분을 lossless codelength라는 표현을 하며 큰 영향력이 없는 loss여서 이부분도 생락했다고 한다. (사실 마지막에 노이즈 엄청 조금은 있으나 마나하긴하다 실제로 코드에서도 sampling할 때 마지막 term은 그대로 보냈다.)"],"metadata":{"id":"6Hrd65X1CH8Q"}},{"cell_type":"markdown","source":["이제, 중간의 $L_{t-1}$을 자세히 뜯어보자. 여기를 계산하기 위해 두가지를 알아야한다."],"metadata":{"id":"3fuUiFLZCH4C"}},{"cell_type":"markdown","source":["1. $q(x_{t-1}|x_t, x_0)$\n","2. $p_\\theta(x_{t-1}|x_t)$"],"metadata":{"id":"NVY3yCN_CH1b"}},{"cell_type":"markdown","source":["__q구하기__"],"metadata":{"id":"fh05MlQhZjEj"}},{"cell_type":"markdown","source":["먼저, $x_{t-1}$을 $x_t, x_0$에서 비롯된 평균 분산으로부터 sampling하는 것이라고 생각하면, 다음의 식을 구성할 수 있다.\n","\n","(여기선 분산은 스케줄된 것을 이용한다)  \n","$q(x_{t-1}|x_t, x_0) = N(x_{t-1};\\tilde{\\mu_t}(x_t, x_0), \\tilde{\\beta_t}I)$  \n"],"metadata":{"id":"l5b8UeEvCHzN"}},{"cell_type":"markdown","source":["(where)  \n","$\\tilde{\\mu_t}(x_t, x_0) = \\cfrac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_t}\\beta_t x_0 + \\cfrac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} x_t$  \n","$\\tilde{\\beta_t} = \\cfrac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t$"],"metadata":{"id":"uZkxckpPCHxJ"}},{"cell_type":"markdown","source":["q로부터 유도되어, $x_t$에서 $x_0$까지 얼만큼 interpolation한 곳에서 sampling할 것인가 라고 의미적으로 이해할 수 있다"],"metadata":{"id":"TnD5vNqmCHvB"}},{"cell_type":"markdown","source":["__p구하기__"],"metadata":{"id":"OMEIGcIzZmtX"}},{"cell_type":"markdown","source":["그러면, 이 값을 주어진 파라미터 $\\theta$로 함수 $p_\\theta(\\cdot)$를 어떻게 구성할 것인가가 중요한 문제이다."],"metadata":{"id":"Wg3o9wOGCHs3"}},{"cell_type":"markdown","source":["$p_\\theta(x_{t-1}|x_t) = N(x_{t-1};\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$이와 같이 평균과 분산에 대해 파라미터화 할 수 있다."],"metadata":{"id":"BTffsSYLZVRb"}},{"cell_type":"markdown","source":["여기에서도 앞과같이 분산은 학습하지않고 평균만 학습한다고 하면,\n","+ $\\Sigma_\\theta(x_t, t) = \\sigma_t^2I$\n","\n","그리고,\n","+ $\\sigma_t^2 = \\tilde{\\beta_t} = \\cfrac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t} \\beta_t$\n","+ $\\sigma_t^2 = \\beta_t$\n","\n","다음의 관계가 성립함을 실험적으로 알려져 있다. 그러면, 분산을 같게 했으니  \n"," 이제 이 평균을 어떻게 생각하는지가 관건이다. 결국 각각의 평균만 잘 같아지도록 하면된다!!"],"metadata":{"id":"8GYiYJ-8CHqq"}},{"cell_type":"markdown","source":["(수식 유도)  \n","<img src=\"https://drive.google.com/uc?id=18PvMsHAh4VvKUIQlOFmiqwYDLXZlKnTd\" width=600>  \n","<img src=\"https://drive.google.com/uc?id=1ga4TFfOKed84ZWZMnYQhZEGzJBNDNWWh\" width=550>  \n"],"metadata":{"id":"0P_s43TdYdIX"}},{"cell_type":"markdown","source":["이 논문에서 나름 키포인트라고 생각함. 평균을 학습하는 것이아니라, 잔차 ($\\epsilon$을 학습하는 방향으로 유도한다.)   \n","그래서, $\\tilde{\\mu}(x_t, x_0)$ term에서, 이 $x_0$대신에 밑에와 같이 reparameterization trick을 사용할 때의 입실론 값을 학습하고자하는 값으로 잡은 것이다!\n","\n","<img src=\"https://drive.google.com/uc?id=1vEHzIul1idE9IV0bcO0fZR99-fHgpxd1\" width=550>  \n","<img src=\"https://drive.google.com/uc?id=1OPetvGvtNxf5Wn6aZOYMQaQ_mtDEHZFn\" width=600>"],"metadata":{"id":"Zj_oo_uHYdGP"}},{"cell_type":"markdown","source":["최종적으로 단순화한 loss는 다음과 같다.\n","$$L_{simple}(\\theta) = \\mathbb{E}_{t, x_0, \\epsilon}\\bigg[||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t)||^2\\bigg]$$"],"metadata":{"id":"mbJth5ZOYdET"}},{"cell_type":"markdown","source":["추가적으로, 이 논문에선 $\\theta$를 학습하기 위해 U-Net을 이용했다고하고, time step t를 잘 임베딩하기 위해 self-attention기술을 이용했다고 한다."],"metadata":{"id":"4XX8pEqZYdCW"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1w1l3-VYiEOGHRewha7adMM8HC-ZkklG8\" height=200>"],"metadata":{"id":"qHzyD2qLi9Wc"}},{"cell_type":"markdown","source":["sampling과정에서의 denoising이 결과적으로 보니, score기반 Langevin danamics와 동일하다고도 볼 수 있다."],"metadata":{"id":"jzLY-YLxi9Uk"}},{"cell_type":"markdown","source":["## 실습했던 코드들"],"metadata":{"id":"m4AusfMLUrPS"}},{"cell_type":"code","source":["# import cv2\n","# from datetime import datatime\n","\n","import gc, os, math, base64, random\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.cuda import amp\n","\n","import torch.nn.functional as F\n","from torch.optim import Adam, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","import torchvision.transforms as TF\n","import torchvision.datasets as datasets\n","from torchvision.utils import make_grid\n","\n","\n","from IPython.display import display, HTML, clear_output"],"metadata":{"id":"zV3LLi1WokFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SinusoidalPositionEmbeddings(nn.Module):\n","    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n","        super().__init__()\n","\n","        half_dim = time_emb_dims // 2\n","\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n","\n","        ts = torch.arange(total_time_steps, dtype=torch.float32)\n","\n","        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","\n","        self.time_blocks = nn.Sequential(\n","            nn.Embedding.from_pretrained(emb),\n","            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n","            nn.SiLU(),\n","            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n","        )\n","\n","    def forward(self, time):\n","        return self.time_blocks(time)"],"metadata":{"id":"n4orQYM3o8GI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AttentionBlock(nn.Module):\n","    def __init__(self, channels=64):\n","        super().__init__()\n","        self.channels = channels\n","\n","        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n","        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n","\n","    def forward(self, x):\n","        B, _, H, W = x.shape\n","        h = self.group_norm(x)\n","        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)\n","        h, _ = self.mhsa(h, h, h)\n","        h = h.swapaxes(2, 1).view(B, self.channels. H, W)\n","        return x + h"],"metadata":{"id":"Bz0dn9GSo3Ch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResnetBlock(nn.Module):\n","    def __init__(self, *, in_channels, out_channels, dropout_rate=0.1, time_emb_dims=512, apply_attention=False):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","\n","        self.act_fn = nn.SiLU()\n","        # Group1?\n","        self.normalize_1 = nn.GroupNorm(num_groups=8, num_channels=self.in_channels)\n","        self.conv_1 = nn.Conv2d(in_channels=self.in_channels,\n","                                out_channels=self.out_channels, kernel_size=3, padding=\"same\")\n","\n","        # Group2 time embedding\n","        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n","\n","        # Group3\n","        self.normalize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n","        self.dropout = nn.Dropout2d(p=dropout_rate)\n","        self.conv_2 = nn.Conv2d(in_channels=self.out_channels,\n","                                out_channels=self.out_channels, kernel_size=3, padding=\"same\")\n","\n","        if self.in_channels != self.out_channels:\n","            self.match_input = nn.Conv2d(in_channels=self.in_channels,\n","                                         out_channels=out_channels, kernel_size=1, stride=1)\n","        else:\n","            self.match_input = nn.Identity()\n","\n","        if apply_attention:\n","            self.attention = AttentionBlock(channels=self.out_channels)\n","        else:\n","            self.attention = nn.Identity()\n","\n","    def forward(self, x, t):\n","        # Group1\n","        h = self.act_fn(self.normalize_1(x))\n","        h = self.conv_1(h)\n","\n","        # Group2\n","        # add in timestep embedding\n","        h += self.dense_1(self.act_fn(t))[:, :, None, None]    # 2차원짜리 4차원으로(unsqueeze?)\n","\n","        # Group3\n","        h = self.act_fn(self.normalize_2(h))\n","        h = self.dropout(h)\n","        h = self.conv_2(h)\n","\n","        return h"],"metadata":{"id":"pAdMyOTHotFj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DownSample(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.downsample = nn.Conv2d(in_channels=channels, out_channels=channels,\n","                                   kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x, *args):\n","        return self.downsample(x)\n","\n","class UpSample(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","\n","        self.upsample = nn.Sequential(nn.Upsample(scale_factor=2, mode=\"nearest\"),\n","                                     nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n","                                     )\n","\n","    def forward(self, x, *args):\n","        return self.upsample(x)"],"metadata":{"id":"R5NzHZhNotDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class UNet(nn.Module):\n","    def __init__(\n","    self,\n","    input_channels=3,    # RGB 3개에 대한 평균\n","    output_channels=3,\n","    num_res_blocks=2,\n","    base_channels=64,\n","    base_channels_multiples=(1, 2, 2, 4),\n","    apply_attention=(False, False, True, False),\n","    dropout_rate=0.1,\n","    time_multiple=4,\n","    ):\n","        super().__init__()\n","\n","        time_emb_dims_exp = base_channels * time_multiple\n","        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels,\n","                                                           time_emb_dims_exp=time_emb_dims_exp)\n","\n","        self.first = nn.Conv2d(in_channels=input_channels, out_channels=base_channels,\n","                              kernel_size=3, padding=\"same\")\n","\n","        num_resolutions = len(base_channels_multiples)\n","\n","        # encoder of UNet, dimension reduction part\n","        self.encoder_blocks = nn.ModuleList()\n","        curr_channels = [base_channels]\n","        in_channels = base_channels\n","\n","        for level in range(num_resolutions):\n","            out_channels = base_channels * base_channels_multiples[level]\n","\n","            for _ in range(num_res_blocks):\n","\n","                block = ResnetBlock(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                dropout_rate=dropout_rate,\n","                time_emb_dims=time_emb_dims_exp,\n","                apply_attention=apply_attention[level],\n","                )\n","                self. encoder_blocks.append(block)\n","\n","                in_channels = out_channels\n","                curr_channels.append(in_channels)\n","\n","            if level != (num_resolutions - 1):\n","                self.encoder_blocks.append(DownSample(channels=in_channels))\n","                curr_channels.append(in_channels)\n","\n","        # Bottleneck in between\n","        self.bottleneck_blocks = nn.ModuleList(\n","            (\n","                ResnetBlock(\n","                    in_channels=in_channels,\n","                    out_channels=in_channels,\n","                    dropout_rate=dropout_rate,\n","                    time_emb_dims=time_emb_dims_exp,\n","                    apply_attention=True,\n","                ),\n","                ResnetBlock(\n","                    in_channels=in_channels,\n","                    out_channels=in_channels,\n","                    dropout_rate=dropout_rate,\n","                    time_emb_dims=time_emb_dims_exp,\n","                    apply_attention=False,\n","                ),\n","            )\n","        )\n","\n","        # Decoder in UNet, Dimension restoration with skip-connections ?!\n","        self.decoder_blocks = nn.ModuleList()\n","\n","        for level in reversed(range(num_resolutions)):\n","            out_channels = base_channels * base_channels_multiples[level]\n","\n","            for _ in range(num_res_blocks + 1):\n","                encoder_in_channels = curr_channels.pop()     # 이거 맛있다 ㅋㅋ\n","                block = ResnetBlock(\n","                    in_channels=encoder_in_channels + in_channels, # ??? -> 아 이거 concat하니까 당연히 더해야지\n","                    out_channels=out_channels,\n","                    dropout_rate=dropout_rate,\n","                    time_emb_dims=time_emb_dims_exp,\n","                    apply_attention=apply_attention[level],\n","                )\n","\n","                in_channels = out_channels\n","                self.decoder_blocks.append(block)\n","\n","            if level != 0:\n","                self.decoder_blocks.append(UpSample(in_channels))\n","\n","        self.final = nn.Sequential(\n","        nn.GroupNorm(num_groups=8, num_channels=in_channels),\n","        nn.SiLU(),\n","        nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3,\n","                 stride=1, padding=\"same\"),\n","        )\n","    def forward(self, x, t):\n","\n","        time_emb = self.time_embeddings(t)\n","\n","        h = self.first(x)\n","        outs = [h]\n","\n","        for layer in self.encoder_blocks:\n","            h = layer(h, time_emb)\n","            outs.append(h)\n","\n","        for layer in self.bottleneck_blocks:\n","            h = layer(h, time_emb)\n","\n","        for layer in self.decoder_blocks:\n","            if isinstance(layer, ResnetBlock):\n","                out = outs.pop()\n","                h = torch.cat([h, out], dim=1)    # skip-connection\n","            h = layer(h, time_emb)\n","\n","        h = self.final(h)\n","\n","        return h"],"metadata":{"id":"0LKRIMzXoha8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["M = UNet()"],"metadata":{"id":"8H548rh1pIbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = torch.randint(low=1, high=1000, size=(3,))\n","xt = torch.rand((3,3,64,64))"],"metadata":{"id":"Jf93v3Atpcj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_noise = M(xt, t)"],"metadata":{"id":"_ExzCXj5pOCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_noise.shape"],"metadata":{"id":"YE2vdX0trKKW","executionInfo":{"status":"ok","timestamp":1689091110586,"user_tz":-540,"elapsed":465,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"abd28ac6-8c72-48ab-c2bf-cb5761a59e7d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 3, 64, 64])"]},"metadata":{},"execution_count":23}]}]}