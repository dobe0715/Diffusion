{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyMPA+Rh4YN4RvQZxez2jUj7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import sys\n","sys.version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KwpgUCPlw1kc","executionInfo":{"status":"ok","timestamp":1686039190299,"user_tz":-540,"elapsed":17,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"5bf6ed9b-5853-4862-92dd-751996e575ed"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=18wzt4KxmNxVLBaaovJZEmF8VC31206vG\n","\">"],"metadata":{"id":"M2Kyjf72e2ah"}},{"cell_type":"code","source":["!git clone https://github.com/ermongroup/SDEdit.git\n","%cd /content/SDEdit"],"metadata":{"id":"041BPn_W7GyH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685977487108,"user_tz":-540,"elapsed":6621,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"c5925724-ec3e-468b-e7c2-130d99487d17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'SDEdit'...\n","remote: Enumerating objects: 156, done.\u001b[K\n","remote: Counting objects: 100% (155/155), done.\u001b[K\n","remote: Compressing objects: 100% (89/89), done.\u001b[K\n","remote: Total 156 (delta 69), reused 129 (delta 57), pack-reused 1\u001b[K\n","Receiving objects: 100% (156/156), 37.39 MiB | 7.62 MiB/s, done.\n","Resolving deltas: 100% (69/69), done.\n","/content/SDEdit\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2utv7ag6KnpO","executionInfo":{"status":"ok","timestamp":1686055821428,"user_tz":-540,"elapsed":28710,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"ddadaa57-9316-4eee-e99a-7414b5b11fde"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","img_real = plt.imread(\"/content/drive/MyDrive/Capstone/sde_images/mars_man.png\")\n","img_stroke = plt.imread(\"/content/drive/MyDrive/Capstone/sde_images/mars_man_stroke.png\")\n","\n","img_mask = img_real - img_stroke[:, :, :3]\n","img_mask = (1,) - np.array(img_mask != 0, dtype=float)"],"metadata":{"id":"FFNkFeTqyNh4","executionInfo":{"status":"ok","timestamp":1686055828413,"user_tz":-540,"elapsed":2468,"user":{"displayName":"소신","userId":"16241659929656994443"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["img_mask.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tf0QQz9QX-EL","executionInfo":{"status":"ok","timestamp":1686055837193,"user_tz":-540,"elapsed":4,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"e3af1a4b-3ae9-4c2f-e1bd-66a46b024387"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(438, 438, 3)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# import torch\n","\n","# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# url = \"/content/drive/MyDrive/Capstone/celeba_hq.ckpt\"\n","\n","# ckpt = torch.load(url, map_location=device)"],"metadata":{"id":"oRkQ1iti7GuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","\n","def dict2namespace(config):\n","    namespace = argparse.Namespace()\n","    for key, value in config.items():\n","        if isinstance(value, dict):\n","            new_value = dict2namespace(value)\n","        else:\n","            new_value = value\n","        setattr(namespace, key, new_value)\n","    return namespace"],"metadata":{"id":"N_aVjbqTPp9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from models.diffusion import Model\n","# import yaml\n","\n","\n","# config = \"/content/SDEdit/configs/celeba.yml\"\n","\n","# with open(config, 'r') as f:\n","#     config = yaml.safe_load(f)\n","#     new_config = dict2namespace(config)\n","\n","# model = Model(new_config)\n","# model.load_state_dict(ckpt)"],"metadata":{"id":"MS-Bojq57GsG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685644732788,"user_tz":-540,"elapsed":1314,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"c518b313-7a76-4aed-a53b-ef84946ef0a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# def image_editing_sample(self):\n","#     device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","#     url = \"/content/drive/MyDrive/Capstone/celeba_hq.ckpt\"\n","#     ckpt = torch.load(url, map_location=device)\n","\n","#     config = \"/content/SDEdit/configs/celeba.yml\"\n","\n","#     with open(config, 'r') as f:\n","#         config = yaml.safe_load(f)\n","#         new_config = dict2namespace(config)\n","\n","#     model = Model(new_config)\n","#     model.load_state_dict(ckpt)\n","#     model.to(self.device)\n","#     model = torch.nn.DataParallel(model)\n","#     print(\"Model loaded\")\n","#     ckpt_id = 0"],"metadata":{"id":"Ka1ANSTO7GqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yaml\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import torch\n","import torchvision\n","import torchvision.utils as tvu\n","\n","from models.diffusion import Model\n","\n","\n","def get_beta_schedule(*, beta_start, beta_end, num_diffusion_timesteps):\n","    betas = np.linspace(beta_start, beta_end,\n","                        num_diffusion_timesteps, dtype=np.float64)\n","    assert betas.shape == (num_diffusion_timesteps,)\n","    return betas\n","\n","\n","def extract(a, t, x_shape):\n","    \"\"\"Extract coefficients from a based on t and reshape to make it\n","    broadcastable with x_shape.\"\"\"\n","    bs, = t.shape\n","    assert x_shape[0] == bs\n","    out = torch.gather(torch.tensor(a, dtype=torch.float, device=t.device), 0, t.long())\n","    assert out.shape == (bs,)\n","    out = out.reshape((bs,) + (1,) * (len(x_shape) - 1))\n","    return out\n","\n","\n","def image_editing_denoising_step_flexible_mask(x, t, *,\n","                                               model,\n","                                               logvar,\n","                                               betas):\n","    \"\"\"\n","    Sample from p(x_{t-1} | x_t)\n","    \"\"\"\n","    alphas = 1.0 - betas\n","    alphas_cumprod = alphas.cumprod(dim=0)\n","\n","    model_output = model(x, t)\n","    weighted_score = betas / torch.sqrt(1 - alphas_cumprod)\n","    mean = extract(1 / torch.sqrt(alphas), t, x.shape) * (x - extract(weighted_score, t, x.shape) * model_output)\n","\n","    logvar = extract(logvar, t, x.shape)\n","    noise = torch.randn_like(x)\n","    mask = 1 - (t == 0).float()\n","    mask = mask.reshape((x.shape[0],) + (1,) * (len(x.shape) - 1))\n","    sample = mean + mask * torch.exp(0.5 * logvar) * noise\n","    sample = sample.float()\n","    return sample\n","\n","\n","class Diffusion(object):\n","    def __init__(self, args, config, device=None):\n","        self.args = args\n","        self.config = config\n","\n","        with open(self.config, 'r') as f:\n","            config = yaml.safe_load(f)\n","            config = dict2namespace(config)\n","\n","        self.config = config\n","\n","        if device is None:\n","            device = torch.device(\n","                \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","        self.device = device\n","\n","        self.model_var_type = config.model.var_type\n","        betas = get_beta_schedule(\n","            beta_start=config.diffusion.beta_start,\n","            beta_end=config.diffusion.beta_end,\n","            num_diffusion_timesteps=config.diffusion.num_diffusion_timesteps\n","        )\n","        self.betas = torch.from_numpy(betas).float().to(self.device)\n","        self.num_timesteps = betas.shape[0]\n","\n","        alphas = 1.0 - betas\n","        alphas_cumprod = np.cumprod(alphas, axis=0)\n","        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n","        posterior_variance = betas * \\\n","            (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n","        if self.model_var_type == \"fixedlarge\":\n","            self.logvar = np.log(np.append(posterior_variance[1], betas[1:]))\n","\n","        elif self.model_var_type == 'fixedsmall':\n","            self.logvar = np.log(np.maximum(posterior_variance, 1e-20))\n","\n","    def image_editing_sample(self):\n","        print(\"Loading model\")\n","        # if self.config.data.dataset == \"LSUN\":\n","        #     if self.config.data.category == \"bedroom\":\n","        #         url = \"https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt\"\n","        #     elif self.config.data.category == \"church_outdoor\":\n","        #         url = \"https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt\"\n","        # elif self.config.data.dataset == \"CelebA_HQ\":\n","        #     url = \"https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt\"\n","        # else:\n","        #     raise ValueError\n","\n","        # model = Model(self.config)\n","        # ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)\n","        # model.load_state_dict(ckpt)\n","        # model.to(self.device)\n","        # model = torch.nn.DataParallel(model)\n","        # print(\"Model loaded\")\n","        # ckpt_id = 0\n","\n","        # self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","        url = \"/content/drive/MyDrive/Capstone/celeba_hq.ckpt\"\n","        ckpt = torch.load(url, map_location=self.device)\n","\n","        config = \"/content/SDEdit/configs/celeba.yml\"\n","\n","        # with open(config, 'r') as f:\n","        #     config = yaml.safe_load(f)\n","        #     new_config = dict2namespace(config)\n","\n","        # self.config = new_config\n","        \n","        model = Model(self.config)\n","        model.load_state_dict(ckpt)\n","        model.to(self.device)\n","        model = torch.nn.DataParallel(model)\n","        print(\"Model loaded\")\n","        ckpt_id = 0\n","\n","\n","\n","        # download_process_data(path=\"colab_demo\")\n","        # n = self.config.sampling.batch_size\n","        n = 2\n","        model.eval()\n","        print(\"Start sampling\")\n","        with torch.no_grad():\n","            name = self.args.npy_name\n","            # [mask, img] = torch.load(\"colab_demo/{}.pth\".format(name))\n","\n","            resolution = self.config.data.image_size\n","\n","            convert_tensor = torchvision.transforms.ToTensor()\n","            convert_size = torchvision.transforms.Resize((resolution, resolution))\n","\n","            mask = torch.tensor(name)\n","            mask = mask.permute(2, 0, 1)\n","            img = Image.open(\"/content/drive/MyDrive/Capstone/sde_images/mars_man_stroke.png\").convert('RGB')\n","            img = convert_tensor(img)\n","\n","            mask = convert_size(mask)\n","            img = convert_size(img)\n","\n","            mask = mask.to(self.device)\n","            img = img.to(self.device)\n","\n","            img = img.unsqueeze(dim=0)\n","            img = img.repeat(n, 1, 1, 1)\n","            # img = np.array(img.cpu(), dtype=np.uint8)\n","            # x0 = Image.fromarray(img)\n","            x0 = img\n","\n","            tvu.save_image(x0, os.path.join(self.args.image_folder, f'original_input.png'))\n","            x0 = (x0 - 0.5) * 2.\n","\n","            for it in range(self.args.sample_step):\n","                e = torch.randn_like(x0)\n","                total_noise_levels = self.args.t\n","                a = (1 - self.betas).cumprod(dim=0)\n","                x = x0 * a[total_noise_levels - 1].sqrt() + e * (1.0 - a[total_noise_levels - 1]).sqrt()\n","                tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder, f'init_{ckpt_id}.png'))\n","\n","                with tqdm(total=total_noise_levels, desc=\"Iteration {}\".format(it)) as progress_bar:\n","                    for i in reversed(range(total_noise_levels)):\n","                        t = (torch.ones(n) * i).to(self.device)\n","                        x_ = image_editing_denoising_step_flexible_mask(x, t=t, model=model,\n","                                                                        logvar=self.logvar,\n","                                                                        betas=self.betas)\n","                        x = x0 * a[i].sqrt() + e * (1.0 - a[i]).sqrt()\n","                        x[:, (mask != 1.)] = x_[:, (mask != 1.)]\n","                        # added intermediate step vis\n","                        if (i - 99) % 100 == 0:\n","                            tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,\n","                                                                       f'noise_t_{i}_{it}.png'))\n","                        progress_bar.update(1)\n","\n","                x0[:, (mask != 1.)] = x[:, (mask != 1.)]\n","                torch.save(x, os.path.join(self.args.image_folder,\n","                                           f'samples_{it}.pth'))\n","                tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,\n","                                                           f'samples_{it}.png'))"],"metadata":{"id":"GA6yEqsK7Gn1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torchvision.utils as vutils\n","\n","# # Create a tensor of shape (3, 64, 64) representing a single RGB image\n","# img = torch.randn(3, 64, 64)\n","# img = img.unsqueeze(dim=0)\n","# img = img.repeat(8, 1, 1, 1)\n","\n","# # Save the image to a file\n","# vutils.save_image(img, 'my_image.png')"],"metadata":{"id":"HUWuC_7Kx4Pi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from PIL import Image\n","# import torchvision\n","\n","# convert_tensor = torchvision.transforms.ToTensor()\n","\n","\n","# img = Image.open(\"/content/drive/MyDrive/Capstone/sde_images/mars_man_stroke.png\")\n","# img = convert_tensor(img)\n","# img = torchvision.transforms.Resize((256, 256))(img)\n","# img = img.unsqueeze(dim=0)\n","\n","# img.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoSXYvz2rx7l","executionInfo":{"status":"ok","timestamp":1685977542554,"user_tz":-540,"elapsed":375,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"5ee5b799-819c-4bfb-cc01-085d102939d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 4, 256, 256])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import easydict\n","# import argparse\n","import traceback\n","import shutil\n","import logging\n","import yaml\n","import sys\n","import os\n","import torch\n","import numpy as np\n","# import torch.utils.tensorboard as tb\n","import copy\n","\n","\n","\n","def make_parse_args(img_msk):\n","    args = easydict.EasyDict({'seed': 1234, \n","                              'exp': 'exp', \n","                              'comment': '', \n","                              'verbose': 'info', \n","                              'sample': 'store_true', \n","                              'i': 'images', \n","                              'image_folder': 'images', \n","                              'ni': 'store_true', \n","                              'npy_name': img_msk, \n","                              'sample_step': 3, \n","                              't': 400})\n","\n","    level = getattr(logging, args.verbose.upper(), None)\n","    if not isinstance(level, int):\n","        raise ValueError('level {} not supported'.format(args.verbose))\n","\n","    handler1 = logging.StreamHandler()\n","    formatter = logging.Formatter('%(levelname)s - %(filename)s - %(asctime)s - %(message)s')\n","    handler1.setFormatter(formatter)\n","    logger = logging.getLogger()\n","    logger.addHandler(handler1)\n","    logger.setLevel(level)\n","\n","    os.makedirs(os.path.join(args.exp, 'image_samples'), exist_ok=True)\n","    args.image_folder = os.path.join(args.exp, 'image_samples', args.image_folder)\n","    if not os.path.exists(args.image_folder):\n","        os.makedirs(args.image_folder)\n","    else:\n","        overwrite = False\n","        if args.ni:\n","            overwrite = True\n","        else:\n","            response = input(\"Image folder already exists. Overwrite? (Y/N)\")\n","            if response.upper() == 'Y':\n","                overwrite = True\n","\n","        if overwrite:\n","            shutil.rmtree(args.image_folder)\n","            os.makedirs(args.image_folder)\n","        else:\n","            print(\"Output image folder exists. Program halted.\")\n","            sys.exit(0)\n","\n","    # add device\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    logging.info(\"Using device: {}\".format(device))\n","\n","    # set random seed\n","    torch.manual_seed(args.seed)\n","    np.random.seed(args.seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","    torch.backends.cudnn.benchmark = True\n","\n","    return args"],"metadata":{"id":"qc8VOs-A7GhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = \"/content/SDEdit/configs/celeba.yml\"\n","args = make_parse_args(img_mask)"],"metadata":{"id":"gAbz73tL7Ge6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685978013109,"user_tz":-540,"elapsed":5,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"b9fa1c8d-0dc3-4606-d635-5c5ee247c4c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Using device: cuda\n","INFO - <ipython-input-11-20b5aa3c5865> - 2023-06-05 15:13:32,851 - Using device: cuda\n","INFO - <ipython-input-11-20b5aa3c5865> - 2023-06-05 15:13:32,851 - Using device: cuda\n"]}]},{"cell_type":"code","source":["try:\n","    runner = Diffusion(args, config)\n","    runner.image_editing_sample()\n","except Exception:\n","    logging.error(traceback.format_exc())"],"metadata":{"id":"NtC023_07GWB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685978109294,"user_tz":-540,"elapsed":95285,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"0be99024-4670-4fad-a216-94c7d582ac33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model\n","Model loaded\n","Start sampling\n"]},{"output_type":"stream","name":"stderr","text":["Iteration 0:   0%|          | 0/400 [00:00<?, ?it/s]<ipython-input-10-ffd958c84eb2>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  out = torch.gather(torch.tensor(a, dtype=torch.float, device=t.device), 0, t.long())\n","Iteration 0: 100%|██████████| 400/400 [00:32<00:00, 12.14it/s]\n","Iteration 1: 100%|██████████| 400/400 [00:29<00:00, 13.38it/s]\n","Iteration 2: 100%|██████████| 400/400 [00:30<00:00, 13.27it/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cnxzEJJ3Ht6m"},"execution_count":null,"outputs":[]}]}