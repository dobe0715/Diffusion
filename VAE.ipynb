{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO3DwvrJ994PuJmXYME9MNc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# VAE\n","(송경우교수님 딥러닝강의 2022) https://www.youtube.com/watch?v=V-lWbJtNzTc&list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&index=58"],"metadata":{"id":"x4CzM1YSaee2"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1d4eR5ta0-gOjods4S60vOyR_IUb-oLuD\" height=\"300\">"],"metadata":{"id":"LlVmzwaIaecv"}},{"cell_type":"markdown","source":["## Generative model\n","\n","+ MLE의 관점에서 보았을 때, $p_\\theta(x)$를 최대화 하는 파라미터를 찾는 것이다!"],"metadata":{"id":"U3YjBj14aeaU"}},{"cell_type":"markdown","source":["결국 목적함수는,   \n","$$\\theta^* = \\arg\\max\\limits_{\\theta}\\cfrac{1}{N}\\sum_{i=1}^N\\log{p_\\theta(x_i)}$$  \n","\n","이다."],"metadata":{"id":"lZgiiEjBaeUn"}},{"cell_type":"markdown","source":["#### Variational Inference\n","+ 어떤 조건이 주어졌을 때의 확률($p(z|x)$)을 다루기 쉬운 확률분포($q(z)$)로 근사하는 것."],"metadata":{"id":"TG4aYDJlaeR8"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1ryponrQU_kCjVlBcexogjeET47CrquS9\" width=600>"],"metadata":{"id":"POywmQ3kaePu"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1moaCYsW9Tyy0SDBFMTNZtXXTfQ3wjaZ_\" width=800>"],"metadata":{"id":"fyE0ZAz-ZcRt"}},{"cell_type":"markdown","source":["즉, log-likelihood($\\log{p(x_i)}$)의 lower bound인 $L$를 maximize하면 결국에 $q_i(z)$가 $p(z|x_i)$와 가까워져, 실제 샘플에 대응하는 latent를 더 잘 뽑아줄 수 있게 된다."],"metadata":{"id":"OlVGy5C0aP1a"}},{"cell_type":"markdown","source":["이를 학습하기 위해 elbo를 maximize 하는 과정을 살펴보면,  \n","i번째 샘플 데이터학습할 때 마다, $q_i$로부터 $\\mu_i, \\Sigma_i$를 얻어내고, 여기로부터의 분포에서 다시 $\\hat{x}$를 뽑아내서($\\theta$), $x_i$와 닮도록 학습한다.  \n","\n","이 때, 각 데이터 샘플마다 뮤와 시그마에 대응시키는 파라미터가 필요하다.  \n","-> 너무 많다... 새로운 데이터 들어올때마다 또 추가해야한다."],"metadata":{"id":"53UKVlCBaPxy"}},{"cell_type":"markdown","source":["#### Amortized Variational Inference\n","+ $x_i$에 대응되는 $z_i$가 존재하는 상황\n","    + N개의 데이터가 있다면,,, 원래는 N개의 파라미터를 대응시켜서 학습시켰다..(n개의 튜플만들어서 각 튜플마다 파라미터로서 바꾸는느낌)\n","    + 이걸 차라리 하나의 네트워크를 통해 mapping을 시켜주겠다!!(보통 DNN에서 입력, 출력 원하는 방향으로 하듯이)"],"metadata":{"id":"B3qEu6chaPt7"}},{"cell_type":"markdown","source":["수식으로 보면, $q_i(z) \\approx q(z|x_i)$ 왼쪽거 대신 오른쪽거로 VI를 하겠다는 뜻\n","데이터 하나마다 학습되는 과정을 수식으로 바라보면,  \n","$x_i$-> NN($q_\\phi(z|x)$) -> $\\mu(x_i), \\sigma(x_i)$ -> $z=\\mu(x_i) + \\epsilon\\sigma(x_i)$ -> NN($p_\\theta(x|z)$) -> $\\hat{x} \\approx x$  \n","<img src=\"https://drive.google.com/uc?id=1d4eR5ta0-gOjods4S60vOyR_IUb-oLuD\" height=\"300\">"],"metadata":{"id":"vBOAN7EsaPq7"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1y2FU4IzeWUANlmsDTDfc6rEsdfnaGJT6\" height=400>"],"metadata":{"id":"oBtV5H6laPn8"}},{"cell_type":"markdown","source":["## 코드 실습"],"metadata":{"id":"z4p8PzGpaeEq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4l-OiqDaSsh","executionInfo":{"status":"ok","timestamp":1687914371662,"user_tz":-540,"elapsed":328,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"48703c57-be96-456a-9f7f-7eb7cd2d8abe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":116}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","from PIL import Image\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","source":["data_set = torchvision.datasets.MNIST('./data',\n","                                      train=True,\n","                                      transform=transforms.Compose([\n","                                          transforms.Resize((32, 32)),\n","                                          transforms.ToTensor()\n","                                          ]),\n","                                      download=True,\n","                                      )"],"metadata":{"id":"GJ9LwpCvc0Ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_set"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"psYRg8gzypes","executionInfo":{"status":"ok","timestamp":1687914372192,"user_tz":-540,"elapsed":12,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"a1f5418e-992b-499b-f66f-75e1e7bdc08d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset MNIST\n","    Number of datapoints: 60000\n","    Root location: ./data\n","    Split: Train\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n","               ToTensor()\n","           )"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["data_loader = torch.utils.data.DataLoader(dataset=data_set, batch_size=100)"],"metadata":{"id":"3Fd5GSTpfAv1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 모델구성\n","+ mnist data : 28x28\n","+ network :\n","channel : 4, 8, 16, 32 /\n","WxH : 28-14-7-4-2 -> fc1 4 -> 1(mu), fc2 4 -> 1(var)"],"metadata":{"id":"cK26829gqvDj"}},{"cell_type":"code","source":["class VAE(nn.Module):\n","\n","    def __init__(self,\n","                 in_channels,\n","                 latent_dim,\n","                 hidden_dims = None,\n","                 **kwargs):\n","        super(VAE, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","\n","        modules = []\n","        if hidden_dims is None:\n","            hidden_dims = [16, 32, 64, 128]\n","\n","        # Build Encoder\n","        for h_dim in hidden_dims:\n","            modules.append(\n","                nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels=h_dim,\n","                              kernel_size=3, stride=2, padding=1),\n","                    nn.BatchNorm2d(h_dim),\n","                    nn.LeakyReLU()\n","                )\n","            )\n","            in_channels = h_dim\n","\n","        self.encoder = nn.Sequential(*modules)\n","        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n","        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n","\n","        # Build Decoder\n","        modules = []\n","\n","        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4)\n","\n","        hidden_dims.reverse()\n","\n","        for i in range(len(hidden_dims) - 1):\n","            modules.append(\n","                nn.Sequential(\n","                    nn.ConvTranspose2d(hidden_dims[i],\n","                                       hidden_dims[i+1],\n","                                       kernel_size=3,\n","                                       stride=2,\n","                                       padding=1,\n","                                       output_padding=1),\n","                    nn.BatchNorm2d(hidden_dims[i+1]),\n","                    nn.LeakyReLU()\n","                )\n","            )\n","\n","        self.decoder = nn.Sequential(*modules)\n","\n","        self.final_layer = nn.Sequential(\n","            nn.ConvTranspose2d(hidden_dims[-1],\n","                               hidden_dims[-1],\n","                               kernel_size=3,\n","                               stride=2,\n","                               padding=1,\n","                               output_padding=1),\n","            nn.BatchNorm2d(hidden_dims[-1]),\n","            nn.LeakyReLU(),\n","            nn.Conv2d(hidden_dims[-1], out_channels=1,\n","                      kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def encode(self, input):\n","        result = self.encoder(input)\n","        result = torch.flatten(result, start_dim=1)\n","\n","        mu = self.fc_mu(result)\n","        log_var = self.fc_var(result)\n","\n","        return [mu, log_var]\n","\n","    def decode(self, z):\n","        result = self.decoder_input(z)\n","        result = result.view(-1, 128, 2, 2)\n","        result = self.decoder(result)\n","        result = self.final_layer(result)\n","        return result\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return eps*std + mu\n","\n","    def loss_function(self,\n","                      *args):\n","        recons = args[0]\n","        input = args[1]\n","        mu = args[2]\n","        log_var = args[3]\n","\n","        recons_loss = F.mse_loss(recons, input)\n","        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0)\n","\n","        loss = recons_loss + kld_loss\n","\n","        return loss\n","\n","\n","    def sample(self,\n","               num_samples,\n","               current_device,\n","               **kwargs):\n","\n","        z = torch.randn(num_samples,\n","                        self.latent_dim)\n","\n","        z = z.to(current_device)\n","\n","        samples = self.decode(z)\n","        return samples\n","\n","    def generate(self, x, **kwargs):\n","        return self.forward(x)[0]"],"metadata":{"id":"WKEL3SsxdP8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = VAE(in_channels=1, latent_dim=200).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))"],"metadata":{"id":"6Onft-Nnjdho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num = 0\n","for epoch in range(30):\n","    for i, (images, _) in enumerate(data_loader):\n","        # forward\n","        x = images.to(device)\n","        mu, log_var = model.encode(x)\n","        z = model.reparameterize(mu, log_var)\n","\n","        x_rec = model.decode(z)\n","\n","        # compute loss\n","        loss = model.loss_function(x_rec, x, mu, log_var)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % 100 == 0:\n","            print(f\"epoch : {epoch+1} | iter : {i} | loss : {loss:.4}\")\n","\n","        if i % 200 == 0:\n","            samples = model.sample(1, device)\n","            save_image(samples, f\"./vae_samples/sample{num}.png\")\n","        num += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HrbnzjhwioU","executionInfo":{"status":"ok","timestamp":1687921115321,"user_tz":-540,"elapsed":447004,"user":{"displayName":"소신","userId":"16241659929656994443"}},"outputId":"25b4648f-7509-481e-a5b5-3af3a57c70de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch : 1 | iter : 0 | loss : 25.12\n","epoch : 1 | iter : 100 | loss : 0.2628\n","epoch : 1 | iter : 200 | loss : 0.2504\n","epoch : 1 | iter : 300 | loss : 0.1924\n","epoch : 1 | iter : 400 | loss : 0.1042\n","epoch : 1 | iter : 500 | loss : 0.2347\n","epoch : 2 | iter : 0 | loss : 0.08283\n","epoch : 2 | iter : 100 | loss : 0.06895\n","epoch : 2 | iter : 200 | loss : 0.07953\n","epoch : 2 | iter : 300 | loss : 0.07523\n","epoch : 2 | iter : 400 | loss : 0.07775\n","epoch : 2 | iter : 500 | loss : 0.08221\n","epoch : 3 | iter : 0 | loss : 0.0736\n","epoch : 3 | iter : 100 | loss : 0.06282\n","epoch : 3 | iter : 200 | loss : 0.07073\n","epoch : 3 | iter : 300 | loss : 0.05984\n","epoch : 3 | iter : 400 | loss : 0.06127\n","epoch : 3 | iter : 500 | loss : 0.06909\n","epoch : 4 | iter : 0 | loss : 0.06461\n","epoch : 4 | iter : 100 | loss : 0.06142\n","epoch : 4 | iter : 200 | loss : 0.07161\n","epoch : 4 | iter : 300 | loss : 0.05889\n","epoch : 4 | iter : 400 | loss : 0.05847\n","epoch : 4 | iter : 500 | loss : 0.06437\n","epoch : 5 | iter : 0 | loss : 0.06036\n","epoch : 5 | iter : 100 | loss : 0.06125\n","epoch : 5 | iter : 200 | loss : 0.07911\n","epoch : 5 | iter : 300 | loss : 0.06195\n","epoch : 5 | iter : 400 | loss : 0.05938\n","epoch : 5 | iter : 500 | loss : 0.06209\n","epoch : 6 | iter : 0 | loss : 0.07897\n","epoch : 6 | iter : 100 | loss : 0.06053\n","epoch : 6 | iter : 200 | loss : 0.07334\n","epoch : 6 | iter : 300 | loss : 0.07139\n","epoch : 6 | iter : 400 | loss : 0.06082\n","epoch : 6 | iter : 500 | loss : 0.05961\n","epoch : 7 | iter : 0 | loss : 0.0998\n","epoch : 7 | iter : 100 | loss : 0.05946\n","epoch : 7 | iter : 200 | loss : 0.06843\n","epoch : 7 | iter : 300 | loss : 0.06251\n","epoch : 7 | iter : 400 | loss : 0.05751\n","epoch : 7 | iter : 500 | loss : 0.05803\n","epoch : 8 | iter : 0 | loss : 0.09001\n","epoch : 8 | iter : 100 | loss : 0.05908\n","epoch : 8 | iter : 200 | loss : 0.06798\n","epoch : 8 | iter : 300 | loss : 0.05804\n","epoch : 8 | iter : 400 | loss : 0.05706\n","epoch : 8 | iter : 500 | loss : 0.05436\n","epoch : 9 | iter : 0 | loss : 0.07401\n","epoch : 9 | iter : 100 | loss : 0.0589\n","epoch : 9 | iter : 200 | loss : 0.06516\n","epoch : 9 | iter : 300 | loss : 0.05802\n","epoch : 9 | iter : 400 | loss : 0.07783\n","epoch : 9 | iter : 500 | loss : 0.05494\n","epoch : 10 | iter : 0 | loss : 0.06096\n","epoch : 10 | iter : 100 | loss : 0.05923\n","epoch : 10 | iter : 200 | loss : 0.06546\n","epoch : 10 | iter : 300 | loss : 0.05692\n","epoch : 10 | iter : 400 | loss : 0.05633\n","epoch : 10 | iter : 500 | loss : 0.05238\n","epoch : 11 | iter : 0 | loss : 0.05586\n","epoch : 11 | iter : 100 | loss : 0.05847\n","epoch : 11 | iter : 200 | loss : 0.0642\n","epoch : 11 | iter : 300 | loss : 0.05614\n","epoch : 11 | iter : 400 | loss : 0.05614\n","epoch : 11 | iter : 500 | loss : 0.05214\n","epoch : 12 | iter : 0 | loss : 0.05507\n","epoch : 12 | iter : 100 | loss : 0.05837\n","epoch : 12 | iter : 200 | loss : 0.06383\n","epoch : 12 | iter : 300 | loss : 0.05589\n","epoch : 12 | iter : 400 | loss : 0.05605\n","epoch : 12 | iter : 500 | loss : 0.05205\n","epoch : 13 | iter : 0 | loss : 0.05482\n","epoch : 13 | iter : 100 | loss : 0.05831\n","epoch : 13 | iter : 200 | loss : 0.06379\n","epoch : 13 | iter : 300 | loss : 0.05581\n","epoch : 13 | iter : 400 | loss : 0.05601\n","epoch : 13 | iter : 500 | loss : 0.05198\n","epoch : 14 | iter : 0 | loss : 0.05462\n","epoch : 14 | iter : 100 | loss : 0.05825\n","epoch : 14 | iter : 200 | loss : 0.06375\n","epoch : 14 | iter : 300 | loss : 0.05574\n","epoch : 14 | iter : 400 | loss : 0.05597\n","epoch : 14 | iter : 500 | loss : 0.052\n","epoch : 15 | iter : 0 | loss : 0.05447\n","epoch : 15 | iter : 100 | loss : 0.05825\n","epoch : 15 | iter : 200 | loss : 0.06369\n","epoch : 15 | iter : 300 | loss : 0.05568\n","epoch : 15 | iter : 400 | loss : 0.05597\n","epoch : 15 | iter : 500 | loss : 0.05191\n","epoch : 16 | iter : 0 | loss : 0.0544\n","epoch : 16 | iter : 100 | loss : 0.05823\n","epoch : 16 | iter : 200 | loss : 0.06365\n","epoch : 16 | iter : 300 | loss : 0.0556\n","epoch : 16 | iter : 400 | loss : 0.05592\n","epoch : 16 | iter : 500 | loss : 0.0519\n","epoch : 17 | iter : 0 | loss : 0.0543\n","epoch : 17 | iter : 100 | loss : 0.05822\n","epoch : 17 | iter : 200 | loss : 0.06361\n","epoch : 17 | iter : 300 | loss : 0.05559\n","epoch : 17 | iter : 400 | loss : 0.0559\n","epoch : 17 | iter : 500 | loss : 0.0519\n","epoch : 18 | iter : 0 | loss : 0.05427\n","epoch : 18 | iter : 100 | loss : 0.05824\n","epoch : 18 | iter : 200 | loss : 0.06361\n","epoch : 18 | iter : 300 | loss : 0.05553\n","epoch : 18 | iter : 400 | loss : 0.05594\n","epoch : 18 | iter : 500 | loss : 0.05185\n","epoch : 19 | iter : 0 | loss : 0.05425\n","epoch : 19 | iter : 100 | loss : 0.05821\n","epoch : 19 | iter : 200 | loss : 0.06357\n","epoch : 19 | iter : 300 | loss : 0.05552\n","epoch : 19 | iter : 400 | loss : 0.05591\n","epoch : 19 | iter : 500 | loss : 0.0519\n","epoch : 20 | iter : 0 | loss : 0.05424\n","epoch : 20 | iter : 100 | loss : 0.05821\n","epoch : 20 | iter : 200 | loss : 0.06349\n","epoch : 20 | iter : 300 | loss : 0.0555\n","epoch : 20 | iter : 400 | loss : 0.0559\n","epoch : 20 | iter : 500 | loss : 0.05188\n","epoch : 21 | iter : 0 | loss : 0.05432\n","epoch : 21 | iter : 100 | loss : 0.05819\n","epoch : 21 | iter : 200 | loss : 0.06353\n","epoch : 21 | iter : 300 | loss : 0.0555\n","epoch : 21 | iter : 400 | loss : 0.05591\n","epoch : 21 | iter : 500 | loss : 0.05189\n","epoch : 22 | iter : 0 | loss : 0.05439\n","epoch : 22 | iter : 100 | loss : 0.05821\n","epoch : 22 | iter : 200 | loss : 0.06355\n","epoch : 22 | iter : 300 | loss : 0.05548\n","epoch : 22 | iter : 400 | loss : 0.0559\n","epoch : 22 | iter : 500 | loss : 0.05187\n","epoch : 23 | iter : 0 | loss : 0.05434\n","epoch : 23 | iter : 100 | loss : 0.0582\n","epoch : 23 | iter : 200 | loss : 0.06354\n","epoch : 23 | iter : 300 | loss : 0.05552\n","epoch : 23 | iter : 400 | loss : 0.05592\n","epoch : 23 | iter : 500 | loss : 0.05189\n","epoch : 24 | iter : 0 | loss : 0.0545\n","epoch : 24 | iter : 100 | loss : 0.05819\n","epoch : 24 | iter : 200 | loss : 0.06357\n","epoch : 24 | iter : 300 | loss : 0.05547\n","epoch : 24 | iter : 400 | loss : 0.05589\n","epoch : 24 | iter : 500 | loss : 0.05189\n","epoch : 25 | iter : 0 | loss : 0.05434\n","epoch : 25 | iter : 100 | loss : 0.05818\n","epoch : 25 | iter : 200 | loss : 0.06356\n","epoch : 25 | iter : 300 | loss : 0.05543\n","epoch : 25 | iter : 400 | loss : 0.0559\n","epoch : 25 | iter : 500 | loss : 0.05189\n","epoch : 26 | iter : 0 | loss : 0.0545\n","epoch : 26 | iter : 100 | loss : 0.05818\n","epoch : 26 | iter : 200 | loss : 0.06354\n","epoch : 26 | iter : 300 | loss : 0.05548\n","epoch : 26 | iter : 400 | loss : 0.05589\n","epoch : 26 | iter : 500 | loss : 0.05191\n","epoch : 27 | iter : 0 | loss : 0.05421\n","epoch : 27 | iter : 100 | loss : 0.05817\n","epoch : 27 | iter : 200 | loss : 0.06355\n","epoch : 27 | iter : 300 | loss : 0.0555\n","epoch : 27 | iter : 400 | loss : 0.05591\n","epoch : 27 | iter : 500 | loss : 0.05192\n","epoch : 28 | iter : 0 | loss : 0.05429\n","epoch : 28 | iter : 100 | loss : 0.05816\n","epoch : 28 | iter : 200 | loss : 0.06354\n","epoch : 28 | iter : 300 | loss : 0.05548\n","epoch : 28 | iter : 400 | loss : 0.0559\n","epoch : 28 | iter : 500 | loss : 0.0519\n","epoch : 29 | iter : 0 | loss : 0.05418\n","epoch : 29 | iter : 100 | loss : 0.05819\n","epoch : 29 | iter : 200 | loss : 0.06356\n","epoch : 29 | iter : 300 | loss : 0.05547\n","epoch : 29 | iter : 400 | loss : 0.05594\n","epoch : 29 | iter : 500 | loss : 0.05189\n","epoch : 30 | iter : 0 | loss : 0.05412\n","epoch : 30 | iter : 100 | loss : 0.05817\n","epoch : 30 | iter : 200 | loss : 0.06355\n","epoch : 30 | iter : 300 | loss : 0.05546\n","epoch : 30 | iter : 400 | loss : 0.05589\n","epoch : 30 | iter : 500 | loss : 0.05189\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"w01op0pm7mpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class VAE(nn.Module):\n","\n","#     def __init__(self,\n","#                  input_feature,\n","#                  latent_dim,\n","#                  hidden_dim,\n","#                  **kwargs):\n","#         super(VAE, self).__init__()\n","\n","\n","#         # Build Encoder\n","#         self.e_fc1 = nn.Sequential(\n","#             nn.Linear(input_feature, hidden_dim),\n","#             nn.BatchNorm2d(hidden_dim),\n","#             nn.LeakyReLU(0.2)\n","#             )\n","\n","#         self.e_fc2 = nn.Sequential(\n","#             nn.Linear(hidden_dim, hidden_dim),\n","#             nn.BatchNorm2d(hidden_dim),\n","#             nn.LeakyReLU(0.2)\n","#             )\n","\n","#         # output layer\n","#         self.mu = nn.Linear(hidden_dim, latent_dim)\n","#         self.var = nn.Linear(hidden_dim, latent_dim)\n","\n","\n","\n","#         # Build Decoder\n","\n","#         self.d_fc1 = nn.Sequential(\n","#             nn.Linear(latent_dim, hidden_dim),\n","#             nn.BatchNorm2d(hidden_dim),\n","#             nn.LeakyReLU(0.2)\n","#             )\n","\n","#         self.d_fc2 = nn.Sequential(\n","#             nn.Linear(hidden_dim, hidden_dim),\n","#             nn.BatchNorm2d(hidden_dim),\n","#             nn.LeakyReLU(0.2)\n","#             )\n","\n","#         self.fianl_layer = nn.Sequential(\n","#             nn.Linear(hidden_dim, input_feature),\n","#             nn.Tanh()\n","#         )\n","\n","\n","#     def encode(self, input):\n","#         result = self.e_fc1(input)\n","#         result = self.e_fc2(result)\n","\n","#         mu = self.mu(result)\n","#         log_var = self.var(result)\n","\n","#         return [mu, log_var]\n","\n","#     def decode(self, z):\n","#         result = self.d_fc1(z)\n","#         result = self.d_fc2(result)\n","#         result = self.final_layer(result)\n","#         return result\n","\n","#     def reparameterize(self, mu, logvar):\n","#         std = torch.exp(0.5*logvar)\n","#         eps = torch.randn_like(std)\n","#         return eps*std + mu\n","\n","#     def loss_function(self,\n","#                       *args):\n","#         recons = args[0]\n","#         input = args[1]\n","#         mu = args[2]\n","#         log_var = args[3]\n","\n","#         recons_loss = F.mse_loss(recons, input)\n","#         kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0)\n","\n","#         loss = recons_loss + kld_loss\n","\n","#         return loss\n","\n","\n","#     def sample(self,\n","#                num_samples,\n","#                current_device,\n","#                **kwargs):\n","\n","#         z = torch.randn(num_samples,\n","#                         self.latent_dim)\n","\n","#         z = z.to(current_device)\n","\n","#         samples = self.decode(z)\n","#         samples = samples.view(-1, 28, 28)\n","#         return samples\n","\n","#     def generate(self, x, **kwargs):\n","#         return self.forward(x)[0]"],"metadata":{"id":"WDbg35EjHx9W"},"execution_count":null,"outputs":[]}]}